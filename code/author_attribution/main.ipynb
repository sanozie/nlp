{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samuel Anozie\n",
    "## Author Attribution\n",
    "Attributing authors to writers of the Federalist papers, documents arguing the correct structure of the US Constitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data via pandas, and displaying prelimiary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author                                               text\n",
      "0  HAMILTON  FEDERALIST. No. 1 General Introduction For the...\n",
      "1       JAY  FEDERALIST No. 2 Concerning Dangers from Forei...\n",
      "2       JAY  FEDERALIST No. 3 The Same Subject Continued (C...\n",
      "3       JAY  FEDERALIST No. 4 The Same Subject Continued (C...\n",
      "4       JAY  FEDERALIST No. 5 The Same Subject Continued (C...\n",
      "HAMILTON                49\n",
      "MADISON                 15\n",
      "HAMILTON OR MADISON     11\n",
      "JAY                      5\n",
      "HAMILTON AND MADISON     3\n",
      "Name: author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('federalist.csv', usecols=[0,1], header=0)\n",
    "df[\"author\"] = pd.Categorical(df.author)\n",
    "print(df.head(5))\n",
    "author_counts = df.author.value_counts()\n",
    "print(author_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing into training and testing sets, and displaying the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66,)\n",
      "(17,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.text\n",
    "y = df.author\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vectorizor to derive meaning from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 7876)\n",
      "(17, 7876)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Naive Bayes to predict authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    HAMILTON       0.59      1.00      0.74        10\n",
      "\n",
      "   micro avg       0.59      1.00      0.74        10\n",
      "   macro avg       0.59      1.00      0.74        10\n",
      "weighted avg       0.59      1.00      0.74        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report\n",
    "naive_bayes = BernoulliNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "#output\n",
    "BernoulliNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "# make predictions on the test data\n",
    "pred = naive_bayes.predict(X_test)\n",
    "# print confusion matrix\n",
    "print(classification_report(y_test, pred, labels=np.unique(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes did not work well with the current settings. Redo with max-vectorization set to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 1000)\n",
      "(17, 1000)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           HAMILTON       0.91      1.00      0.95        10\n",
      "HAMILTON OR MADISON       1.00      1.00      1.00         3\n",
      "                JAY       1.00      0.50      0.67         2\n",
      "            MADISON       1.00      1.00      1.00         2\n",
      "\n",
      "           accuracy                           0.94        17\n",
      "          macro avg       0.98      0.88      0.90        17\n",
      "       weighted avg       0.95      0.94      0.93        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=1000, ngram_range=(1,2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "naive_bayes = BernoulliNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "#output\n",
    "BernoulliNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "# make predictions on the test data\n",
    "pred = naive_bayes.predict(X_test)\n",
    "# print confusion matrix\n",
    "print(classification_report(y_test, pred, labels=np.unique(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes did much better with the new settings. We can now try logistic regression to see if this algorithm can preform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.5882352941176471\n",
      "precision score:  0.14705882352941177\n",
      "recall score:  0.25\n",
      "f1 score:  0.18518518518518517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stopwords, binary=True)),\n",
    "        ('logreg', LogisticRegression(solver='lbfgs')),\n",
    "])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "pred = pipe1.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred, average='macro'))\n",
    "print('recall score: ', recall_score(y_test, pred, average='macro'))\n",
    "print('f1 score: ', f1_score(y_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression was not the best algorithm to use. Nerual Networks may preform better in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7647058823529411\n",
      "precision score:  0.35714285714285715\n",
      "recall score:  0.5\n",
      "f1 score:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, binary=True)\n",
    "X = vectorizer.fit_transform(df.text)\n",
    "y = df.author\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, random_state=1234)\n",
    "\n",
    "\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(15, 2), random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred, average='macro'))\n",
    "print('recall score: ', recall_score(y_test, pred, average='macro'))\n",
    "print('f1 score: ', f1_score(y_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around with different topologies, I was unable to get a result better than 15, 2 for the hidden nodes. Below is one that got me the closest in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7058823529411765\n",
      "precision score:  0.3106060606060606\n",
      "recall score:  0.5\n",
      "f1 score:  0.3630952380952381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(10, 8, 5), random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "print('accuracy score: ', accuracy_score(y_test, pred))\n",
    "print('precision score: ', precision_score(y_test, pred, average='macro'))\n",
    "print('recall score: ', recall_score(y_test, pred, average='macro'))\n",
    "print('f1 score: ', f1_score(y_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
