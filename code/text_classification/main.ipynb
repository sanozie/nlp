{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Samuel Anozie\n",
    "## NLP Text Classification\n",
    "### Fall 2022\n",
    "\n",
    "This project classifies text regarding stereotypical keywords."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Load the data into a pandas dataframe, taking relevant feature and target data. Models will be trained on the following data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                            phrase     sex  age      race  \\\n0                          a drain  Female   39     Black   \n1                         a hassle    Male   19     White   \n2                           a pain  Female   32     White   \n3                         a person  Female   65     White   \n4                        a thinker  Female   65     White   \n...                            ...     ...  ...       ...   \n7021                  true to self  Female   47     White   \n7022                  true to self  Female   43     White   \n7023                 i don't know.    Male   32  Hispanic   \n7024  to the radical left: racists    Male   32  Hispanic   \n7025   to the radical left: sexist    Male   32  Hispanic   \n\n                  politics  friendly  trustworthy  confident  competent  \\\n0     1) Extremely Liberal         2            4          3          3   \n1                        4         3            3          3          3   \n2                        4         3            3          2          1   \n3                        3         2            3          2          4   \n4                        3         4            3          5          5   \n...                    ...       ...          ...        ...        ...   \n7021                     6         5            3          5          5   \n7022                     4         2            3          5          5   \n7023                     5         3            3          3          3   \n7024                     5         3            3          3          3   \n7025                     5         3            3          3          3   \n\n      wealthy  \n0           2  \n1           3  \n2           1  \n3           1  \n4           4  \n...       ...  \n7021        3  \n7022        5  \n7023        3  \n7024        3  \n7025        3  \n\n[7026 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>race</th>\n      <th>politics</th>\n      <th>friendly</th>\n      <th>trustworthy</th>\n      <th>confident</th>\n      <th>competent</th>\n      <th>wealthy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a drain</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>Black</td>\n      <td>1) Extremely Liberal</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a hassle</td>\n      <td>Male</td>\n      <td>19</td>\n      <td>White</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a pain</td>\n      <td>Female</td>\n      <td>32</td>\n      <td>White</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a person</td>\n      <td>Female</td>\n      <td>65</td>\n      <td>White</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a thinker</td>\n      <td>Female</td>\n      <td>65</td>\n      <td>White</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7021</th>\n      <td>true to self</td>\n      <td>Female</td>\n      <td>47</td>\n      <td>White</td>\n      <td>6</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7022</th>\n      <td>true to self</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>White</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7023</th>\n      <td>i don't know.</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7024</th>\n      <td>to the radical left: racists</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7025</th>\n      <td>to the radical left: sexist</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>7026 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "stereotypes = pd.read_csv('Stereotypes.csv', usecols=[2, 5, 6, 7, 8, 14, 15, 16, 17, 18], names=[\"phrase\", \"sex\", \"age\", \"race\", \"politics\", \"friendly\", \"trustworthy\", \"confident\", \"competent\", \"wealthy\"], header=2)\n",
    "stereotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean up data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                            phrase     sex  age      race  politics  friendly  \\\n0                          a drain  Female   39     Black         1         2   \n1                         a hassle    Male   19     White         4         3   \n2                           a pain  Female   32     White         4         3   \n3                         a person  Female   65     White         3         2   \n4                        a thinker  Female   65     White         3         4   \n...                            ...     ...  ...       ...       ...       ...   \n7021                  true to self  Female   47     White         6         5   \n7022                  true to self  Female   43     White         4         2   \n7023                 i don't know.    Male   32  Hispanic         5         3   \n7024  to the radical left: racists    Male   32  Hispanic         5         3   \n7025   to the radical left: sexist    Male   32  Hispanic         5         3   \n\n      trustworthy  confident  competent  wealthy  \n0               4          3          3        2  \n1               3          3          3        3  \n2               3          2          1        1  \n3               3          2          4        1  \n4               3          5          5        4  \n...           ...        ...        ...      ...  \n7021            3          5          5        3  \n7022            3          5          5        5  \n7023            3          3          3        3  \n7024            3          3          3        3  \n7025            3          3          3        3  \n\n[7016 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>race</th>\n      <th>politics</th>\n      <th>friendly</th>\n      <th>trustworthy</th>\n      <th>confident</th>\n      <th>competent</th>\n      <th>wealthy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a drain</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>Black</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a hassle</td>\n      <td>Male</td>\n      <td>19</td>\n      <td>White</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a pain</td>\n      <td>Female</td>\n      <td>32</td>\n      <td>White</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a person</td>\n      <td>Female</td>\n      <td>65</td>\n      <td>White</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a thinker</td>\n      <td>Female</td>\n      <td>65</td>\n      <td>White</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7021</th>\n      <td>true to self</td>\n      <td>Female</td>\n      <td>47</td>\n      <td>White</td>\n      <td>6</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7022</th>\n      <td>true to self</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>White</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7023</th>\n      <td>i don't know.</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7024</th>\n      <td>to the radical left: racists</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7025</th>\n      <td>to the radical left: sexist</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>7016 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stereotypes['phrase'].replace('', np.nan, inplace=True)\n",
    "stereotypes.dropna(inplace=True)\n",
    "stereotypes['politics'].replace('1) Extremely Liberal', '1', inplace=True)\n",
    "stereotypes['politics'].replace('7) Extremely Conservative', '7', inplace=True)\n",
    "stereotypes = stereotypes.astype({'politics': 'int'})\n",
    "stereotypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "             phrase     sex  age      race  politics  friendly  trustworthy  \\\n403            book  Female   24     Black         3         4            3   \n5861         stupid  Female   36     White         3         4            2   \n4885           rich    Male   32  Hispanic         5         4            4   \n6256    undesirable    Male   29     White         1         2            1   \n3426        liberal  Female   39     White         4         3            3   \n...             ...     ...  ...       ...       ...       ...          ...   \n6481  untrustworthy    Male   29     White         7         2            4   \n2454   hard working  Female   42     Black         5         5            3   \n1292      difficult    Male   51     White         3         2            3   \n5474          smart    Male   37     White         2         3            3   \n2667       helpless    Male   28     White         5         2            3   \n\n      confident  competent  wealthy  \n403           3          3        3  \n5861          3          1        1  \n4885          5          4        5  \n6256          2          1        1  \n3426          5          5        5  \n...         ...        ...      ...  \n6481          3          5        2  \n2454          5          5        4  \n1292          2          1        3  \n5474          3          3        3  \n2667          2          2        2  \n\n[5612 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phrase</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>race</th>\n      <th>politics</th>\n      <th>friendly</th>\n      <th>trustworthy</th>\n      <th>confident</th>\n      <th>competent</th>\n      <th>wealthy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>403</th>\n      <td>book</td>\n      <td>Female</td>\n      <td>24</td>\n      <td>Black</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5861</th>\n      <td>stupid</td>\n      <td>Female</td>\n      <td>36</td>\n      <td>White</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4885</th>\n      <td>rich</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>Hispanic</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6256</th>\n      <td>undesirable</td>\n      <td>Male</td>\n      <td>29</td>\n      <td>White</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3426</th>\n      <td>liberal</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>White</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6481</th>\n      <td>untrustworthy</td>\n      <td>Male</td>\n      <td>29</td>\n      <td>White</td>\n      <td>7</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2454</th>\n      <td>hard working</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>Black</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1292</th>\n      <td>difficult</td>\n      <td>Male</td>\n      <td>51</td>\n      <td>White</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5474</th>\n      <td>smart</td>\n      <td>Male</td>\n      <td>37</td>\n      <td>White</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2667</th>\n      <td>helpless</td>\n      <td>Male</td>\n      <td>28</td>\n      <td>White</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5612 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test with 80 train / 20 test\n",
    "train,test = train_test_split(stereotypes, test_size=0.2, random_state = 14)\n",
    "# train,val = train_test_split(train, test_size=0.2, random_state = 1234)\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make targets/features with tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/p8n7h8zn2vq6cg5hdpbw5zvw0000gn/T/ipykernel_1211/1680499680.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "2022-12-08 11:29:42.880502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/46/p8n7h8zn2vq6cg5hdpbw5zvw0000gn/T/ipykernel_1211/1680499680.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec=({'phrase': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'sex': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'race': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'politics': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'friendly': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'trustworthy': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'confident': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'competent': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None), 'wealthy': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32, targets=[]):\n",
    "    df = dataframe.copy()\n",
    "    labels = pd.concat([df.pop(x) for x in targets], axis=1)\n",
    "    df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "\n",
    "targets = [\"friendly\", \"trustworthy\", \"confident\", \"competent\", \"wealthy\"]\n",
    "batch_size = 16\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size, targets=targets)\n",
    "# val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size, targets=targets)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size, targets=targets)\n",
    "train_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do smtn else"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize(text):\n",
    "    remove_regex = f'[{re.escape(string.punctuation)}]'\n",
    "    space_regex = '...'\n",
    "    result = tf.strings.lower(text)\n",
    "    result = tf.strings.regex_replace(result, remove_regex, '')\n",
    "    result = tf.strings.regex_replace(result, space_regex, ' ')\n",
    "    return result\n",
    "\n",
    "def get_vectorization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        standardize=normalize,\n",
    "        output_mode='tf_idf')\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    vectorizer.adapt(feature_ds)\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a layer that turns strings into integer indices.\n",
    "    if dtype == 'string':\n",
    "        index = tf.keras.layers.StringLookup(max_tokens=max_tokens)\n",
    "    # Otherwise, create a layer that turns integer values into integer indices.\n",
    "    else:\n",
    "        index = tf.keras.layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Encode the integer indices.\n",
    "    encoder = tf.keras.layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "    # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "    return lambda feature: encoder(index(feature))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "# Text features.\n",
    "def vector_features(headers):\n",
    "    inputs = []\n",
    "    encoded_features = []\n",
    "    for header in headers:\n",
    "        text_col = tf.keras.Input(shape=(1,), name=header, dtype=tf.string)\n",
    "        vectorization_layer = get_vectorization_layer(header, train_ds)\n",
    "        print(vectorization_layer)\n",
    "        encoded_text_col = vectorization_layer(text_col)\n",
    "        # embed_text_col = tf.keras.layers.Embedding(8000, 64, name=\"embedding\")(encoded_text_col)\n",
    "        # encoded_text_col = tf.keras.layers.GlobalAveragePooling1D()(embed_text_col)\n",
    "        encoded_text_col = tf.cast(encoded_text_col, tf.float64)\n",
    "        inputs.append(text_col)\n",
    "        encoded_features.append(encoded_text_col)\n",
    "\n",
    "    return inputs, encoded_features\n",
    "\n",
    "# Text features (embeddings).\n",
    "def embedding_features(headers):\n",
    "    inputs = []\n",
    "    encoded_features = []\n",
    "    for header in headers:\n",
    "        text_col = tf.keras.Input(shape=(1,), name=header, dtype=tf.string)\n",
    "        vectorization_layer = get_vectorization_layer(header, train_ds)\n",
    "        encoded_text_col = vectorization_layer(text_col)\n",
    "        embed_text_col = tf.keras.layers.Embedding(8000, 64, name=\"embedding\")(encoded_text_col)\n",
    "        encoded_text_col = tf.keras.layers.GlobalAveragePooling1D()(embed_text_col)\n",
    "        inputs.append(text_col)\n",
    "        encoded_features.append(encoded_text_col)\n",
    "\n",
    "    return inputs, encoded_features\n",
    "\n",
    "# Numerical features.\n",
    "def numerical_features(headers):\n",
    "    inputs = []\n",
    "    encoded_features = []\n",
    "    for header in headers:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, train_ds)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "\n",
    "    return inputs, encoded_features\n",
    "\n",
    "\n",
    "def numerical_category_features(headers):\n",
    "    inputs = []\n",
    "    encoded_features = []\n",
    "    for header in headers:\n",
    "        num_category_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        num_category_layer = get_category_encoding_layer(name=header,\n",
    "                                                     dataset=train_ds,\n",
    "                                                     dtype='int64',\n",
    "                                                     max_tokens=5)\n",
    "        encoded_num_category_col = num_category_layer(num_category_col)\n",
    "        inputs.append(num_category_col)\n",
    "        encoded_features.append(encoded_num_category_col)\n",
    "\n",
    "    return inputs, encoded_features\n",
    "\n",
    "def category_features(headers):\n",
    "    inputs = []\n",
    "    encoded_features = []\n",
    "    for header in headers:\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "        encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                                     dataset=train_ds,\n",
    "                                                     dtype='string',\n",
    "                                                     max_tokens=5)\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        inputs.append(categorical_col)\n",
    "        encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "    return inputs, encoded_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wowowowowow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7ff438bc7730>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " phrase (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " politics (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " race (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " sex (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 130)         1           ['phrase[0][0]']                 \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " age (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " integer_lookup (IntegerLookup)  (None, 1)           0           ['politics[0][0]']               \n",
      "                                                                                                  \n",
      " string_lookup_1 (StringLookup)  (None, 1)           0           ['race[0][0]']                   \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['sex[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)           (None, 130)          0           ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 1)            3           ['age[0][0]']                    \n",
      "                                                                                                  \n",
      " category_encoding (CategoryEnc  (None, 5)           0           ['integer_lookup[0][0]']         \n",
      " oding)                                                                                           \n",
      "                                                                                                  \n",
      " category_encoding_1 (CategoryE  (None, 5)           0           ['string_lookup_1[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 3)           0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 144)          0           ['tf.cast[0][0]',                \n",
      "                                                                  'normalization[0][0]',          \n",
      "                                                                  'category_encoding[0][0]',      \n",
      "                                                                  'category_encoding_1[0][0]',    \n",
      "                                                                  'category_encoding_2[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          18560       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           2080        ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           2080        ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           2080        ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " friendly (Dense)               (None, 1)            33          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " trustworthy (Dense)            (None, 1)            33          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " confident (Dense)              (None, 1)            33          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " competent (Dense)              (None, 1)            33          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " wealthy (Dense)                (None, 1)            33          ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,409\n",
      "Trainable params: 70,405\n",
      "Non-trainable params: 4\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vector_inputs, vector_encoded = vector_features(['phrase'])\n",
    "number_inputs, number_encoded = numerical_features(['age'])\n",
    "number_category_inputs, number_category_encoded = numerical_category_features(['politics'])\n",
    "category_inputs, category_encoded = category_features(['race', 'sex'])\n",
    "\n",
    "all_inputs = [*vector_inputs, *number_inputs, *number_category_inputs, *category_inputs]\n",
    "all_encoded_features = [*vector_encoded, *number_encoded, *number_category_encoded, *category_encoded]\n",
    "\n",
    "# Combine all of the input layers into one\n",
    "all_features = tf.keras.layers.Concatenate()(all_encoded_features)\n",
    "base_layer = tf.keras.layers.Dense(units='128', activation='relu')(all_features)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for target in targets:\n",
    "    layer = tf.keras.layers.Dense(units='64', activation='relu')(base_layer)\n",
    "    layer = tf.keras.layers.Dense(units='32', activation='relu')(layer)\n",
    "    outputs.append(tf.keras.layers.Dense(units='1', activation='relu', name=target)(layer))\n",
    "\n",
    "model = tf.keras.Model(all_inputs, outputs=outputs)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ventra/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/keras/engine/functional.py:566: UserWarning: Input dict contained keys ['friendly', 'trustworthy', 'confident', 'competent', 'wealthy'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/351 [==============================] - 2s 3ms/step - loss: 234.9487 - friendly_loss: 50.7631 - trustworthy_loss: 46.0773 - confident_loss: 46.1202 - competent_loss: 46.0087 - wealthy_loss: 45.9794 - friendly_root_mean_squared_error: 1.8420 - trustworthy_root_mean_squared_error: 1.6166 - confident_root_mean_squared_error: 1.6337 - competent_root_mean_squared_error: 1.6271 - wealthy_root_mean_squared_error: 1.6200 - val_loss: 228.0979 - val_friendly_loss: 45.8687 - val_trustworthy_loss: 45.5810 - val_confident_loss: 45.5666 - val_competent_loss: 45.5090 - val_wealthy_loss: 45.5727 - val_friendly_root_mean_squared_error: 1.4025 - val_trustworthy_root_mean_squared_error: 1.4426 - val_confident_root_mean_squared_error: 1.4294 - val_competent_root_mean_squared_error: 1.4441 - val_wealthy_root_mean_squared_error: 1.4309\n",
      "Epoch 2/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 219.5542 - friendly_loss: 43.8846 - trustworthy_loss: 43.9543 - confident_loss: 43.9411 - competent_loss: 43.8994 - wealthy_loss: 43.8749 - friendly_root_mean_squared_error: 1.5376 - trustworthy_root_mean_squared_error: 1.5408 - confident_root_mean_squared_error: 1.5405 - competent_root_mean_squared_error: 1.5357 - wealthy_root_mean_squared_error: 1.5395 - val_loss: 224.6176 - val_friendly_loss: 44.6761 - val_trustworthy_loss: 45.1072 - val_confident_loss: 44.7705 - val_competent_loss: 44.9029 - val_wealthy_loss: 45.1609 - val_friendly_root_mean_squared_error: 1.5132 - val_trustworthy_root_mean_squared_error: 1.5132 - val_confident_root_mean_squared_error: 1.5257 - val_competent_root_mean_squared_error: 1.4863 - val_wealthy_root_mean_squared_error: 1.4592\n",
      "Epoch 3/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 216.1715 - friendly_loss: 43.2675 - trustworthy_loss: 43.2579 - confident_loss: 43.1936 - competent_loss: 43.2229 - wealthy_loss: 43.2297 - friendly_root_mean_squared_error: 1.5338 - trustworthy_root_mean_squared_error: 1.5306 - confident_root_mean_squared_error: 1.5331 - competent_root_mean_squared_error: 1.5320 - wealthy_root_mean_squared_error: 1.5322 - val_loss: 229.8102 - val_friendly_loss: 45.6628 - val_trustworthy_loss: 46.5066 - val_confident_loss: 45.9993 - val_competent_loss: 46.0430 - val_wealthy_loss: 45.5984 - val_friendly_root_mean_squared_error: 1.4261 - val_trustworthy_root_mean_squared_error: 1.4060 - val_confident_root_mean_squared_error: 1.3996 - val_competent_root_mean_squared_error: 1.4276 - val_wealthy_root_mean_squared_error: 1.4259\n",
      "Epoch 4/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 214.7191 - friendly_loss: 42.9897 - trustworthy_loss: 42.9569 - confident_loss: 42.8977 - competent_loss: 42.9185 - wealthy_loss: 42.9564 - friendly_root_mean_squared_error: 1.5264 - trustworthy_root_mean_squared_error: 1.5287 - confident_root_mean_squared_error: 1.5246 - competent_root_mean_squared_error: 1.5298 - wealthy_root_mean_squared_error: 1.5326 - val_loss: 223.5467 - val_friendly_loss: 44.6751 - val_trustworthy_loss: 44.9210 - val_confident_loss: 44.6181 - val_competent_loss: 44.6703 - val_wealthy_loss: 44.6623 - val_friendly_root_mean_squared_error: 1.5592 - val_trustworthy_root_mean_squared_error: 1.5848 - val_confident_root_mean_squared_error: 1.5235 - val_competent_root_mean_squared_error: 1.5754 - val_wealthy_root_mean_squared_error: 1.5296\n",
      "Epoch 5/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 213.3857 - friendly_loss: 42.7353 - trustworthy_loss: 42.6978 - confident_loss: 42.6067 - competent_loss: 42.6439 - wealthy_loss: 42.7018 - friendly_root_mean_squared_error: 1.5262 - trustworthy_root_mean_squared_error: 1.5214 - confident_root_mean_squared_error: 1.5210 - competent_root_mean_squared_error: 1.5261 - wealthy_root_mean_squared_error: 1.5301 - val_loss: 225.8246 - val_friendly_loss: 44.9611 - val_trustworthy_loss: 45.1467 - val_confident_loss: 45.4097 - val_competent_loss: 45.2641 - val_wealthy_loss: 45.0430 - val_friendly_root_mean_squared_error: 1.4906 - val_trustworthy_root_mean_squared_error: 1.4801 - val_confident_root_mean_squared_error: 1.4487 - val_competent_root_mean_squared_error: 1.4808 - val_wealthy_root_mean_squared_error: 1.4728\n",
      "Epoch 6/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 211.5624 - friendly_loss: 42.3973 - trustworthy_loss: 42.2715 - confident_loss: 42.2472 - competent_loss: 42.3076 - wealthy_loss: 42.3388 - friendly_root_mean_squared_error: 1.5110 - trustworthy_root_mean_squared_error: 1.5042 - confident_root_mean_squared_error: 1.5068 - competent_root_mean_squared_error: 1.5071 - wealthy_root_mean_squared_error: 1.5133 - val_loss: 226.2912 - val_friendly_loss: 45.1755 - val_trustworthy_loss: 45.3159 - val_confident_loss: 45.6108 - val_competent_loss: 45.0904 - val_wealthy_loss: 45.0986 - val_friendly_root_mean_squared_error: 1.6443 - val_trustworthy_root_mean_squared_error: 1.6448 - val_confident_root_mean_squared_error: 1.6994 - val_competent_root_mean_squared_error: 1.6424 - val_wealthy_root_mean_squared_error: 1.6550\n",
      "Epoch 7/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 210.8201 - friendly_loss: 42.2610 - trustworthy_loss: 42.1409 - confident_loss: 42.0550 - competent_loss: 42.1593 - wealthy_loss: 42.2042 - friendly_root_mean_squared_error: 1.5114 - trustworthy_root_mean_squared_error: 1.5077 - confident_root_mean_squared_error: 1.5062 - competent_root_mean_squared_error: 1.5106 - wealthy_root_mean_squared_error: 1.5170 - val_loss: 223.0557 - val_friendly_loss: 44.5681 - val_trustworthy_loss: 44.6774 - val_confident_loss: 44.5830 - val_competent_loss: 44.6097 - val_wealthy_loss: 44.6175 - val_friendly_root_mean_squared_error: 1.6265 - val_trustworthy_root_mean_squared_error: 1.5999 - val_confident_root_mean_squared_error: 1.6352 - val_competent_root_mean_squared_error: 1.6203 - val_wealthy_root_mean_squared_error: 1.6447\n",
      "Epoch 8/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 209.2487 - friendly_loss: 41.9872 - trustworthy_loss: 41.8334 - confident_loss: 41.7653 - competent_loss: 41.7944 - wealthy_loss: 41.8683 - friendly_root_mean_squared_error: 1.5063 - trustworthy_root_mean_squared_error: 1.5041 - confident_root_mean_squared_error: 1.4993 - competent_root_mean_squared_error: 1.5050 - wealthy_root_mean_squared_error: 1.5110 - val_loss: 223.6579 - val_friendly_loss: 44.6552 - val_trustworthy_loss: 44.7400 - val_confident_loss: 44.7766 - val_competent_loss: 44.7728 - val_wealthy_loss: 44.7134 - val_friendly_root_mean_squared_error: 1.5970 - val_trustworthy_root_mean_squared_error: 1.5581 - val_confident_root_mean_squared_error: 1.5494 - val_competent_root_mean_squared_error: 1.5690 - val_wealthy_root_mean_squared_error: 1.5759\n",
      "Epoch 9/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 207.7366 - friendly_loss: 41.6831 - trustworthy_loss: 41.4974 - confident_loss: 41.4932 - competent_loss: 41.5311 - wealthy_loss: 41.5317 - friendly_root_mean_squared_error: 1.5032 - trustworthy_root_mean_squared_error: 1.5024 - confident_root_mean_squared_error: 1.5000 - competent_root_mean_squared_error: 1.5057 - wealthy_root_mean_squared_error: 1.5102 - val_loss: 228.3357 - val_friendly_loss: 45.5365 - val_trustworthy_loss: 45.8274 - val_confident_loss: 45.9867 - val_competent_loss: 45.6122 - val_wealthy_loss: 45.3730 - val_friendly_root_mean_squared_error: 1.4709 - val_trustworthy_root_mean_squared_error: 1.4564 - val_confident_root_mean_squared_error: 1.4322 - val_competent_root_mean_squared_error: 1.4621 - val_wealthy_root_mean_squared_error: 1.4691\n",
      "Epoch 10/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 205.8143 - friendly_loss: 41.3018 - trustworthy_loss: 41.1038 - confident_loss: 41.0791 - competent_loss: 41.1444 - wealthy_loss: 41.1852 - friendly_root_mean_squared_error: 1.4979 - trustworthy_root_mean_squared_error: 1.4980 - confident_root_mean_squared_error: 1.4903 - competent_root_mean_squared_error: 1.4987 - wealthy_root_mean_squared_error: 1.5024 - val_loss: 227.5395 - val_friendly_loss: 45.4279 - val_trustworthy_loss: 45.3390 - val_confident_loss: 46.1450 - val_competent_loss: 45.4227 - val_wealthy_loss: 45.2048 - val_friendly_root_mean_squared_error: 1.7112 - val_trustworthy_root_mean_squared_error: 1.6897 - val_confident_root_mean_squared_error: 1.7587 - val_competent_root_mean_squared_error: 1.7192 - val_wealthy_root_mean_squared_error: 1.7017\n",
      "Epoch 11/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 205.1170 - friendly_loss: 41.1499 - trustworthy_loss: 40.9217 - confident_loss: 41.0251 - competent_loss: 40.9748 - wealthy_loss: 41.0455 - friendly_root_mean_squared_error: 1.5064 - trustworthy_root_mean_squared_error: 1.5030 - confident_root_mean_squared_error: 1.5001 - competent_root_mean_squared_error: 1.5033 - wealthy_root_mean_squared_error: 1.5085 - val_loss: 225.0069 - val_friendly_loss: 44.8883 - val_trustworthy_loss: 45.0561 - val_confident_loss: 45.0445 - val_competent_loss: 45.0047 - val_wealthy_loss: 45.0134 - val_friendly_root_mean_squared_error: 1.5801 - val_trustworthy_root_mean_squared_error: 1.5411 - val_confident_root_mean_squared_error: 1.5249 - val_competent_root_mean_squared_error: 1.5581 - val_wealthy_root_mean_squared_error: 1.5665\n",
      "Epoch 12/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 203.5567 - friendly_loss: 40.8693 - trustworthy_loss: 40.6349 - confident_loss: 40.6657 - competent_loss: 40.6923 - wealthy_loss: 40.6944 - friendly_root_mean_squared_error: 1.4967 - trustworthy_root_mean_squared_error: 1.4916 - confident_root_mean_squared_error: 1.4911 - competent_root_mean_squared_error: 1.4929 - wealthy_root_mean_squared_error: 1.4982 - val_loss: 224.5957 - val_friendly_loss: 44.8571 - val_trustworthy_loss: 45.0563 - val_confident_loss: 45.0367 - val_competent_loss: 44.8760 - val_wealthy_loss: 44.7695 - val_friendly_root_mean_squared_error: 1.5968 - val_trustworthy_root_mean_squared_error: 1.6064 - val_confident_root_mean_squared_error: 1.5815 - val_competent_root_mean_squared_error: 1.6245 - val_wealthy_root_mean_squared_error: 1.6210\n",
      "Epoch 13/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 201.9855 - friendly_loss: 40.5454 - trustworthy_loss: 40.2896 - confident_loss: 40.3454 - competent_loss: 40.3766 - wealthy_loss: 40.4284 - friendly_root_mean_squared_error: 1.4831 - trustworthy_root_mean_squared_error: 1.4806 - confident_root_mean_squared_error: 1.4691 - competent_root_mean_squared_error: 1.4793 - wealthy_root_mean_squared_error: 1.4899 - val_loss: 225.1584 - val_friendly_loss: 45.0244 - val_trustworthy_loss: 45.1356 - val_confident_loss: 45.1015 - val_competent_loss: 44.9987 - val_wealthy_loss: 44.8983 - val_friendly_root_mean_squared_error: 1.5576 - val_trustworthy_root_mean_squared_error: 1.5396 - val_confident_root_mean_squared_error: 1.5388 - val_competent_root_mean_squared_error: 1.5493 - val_wealthy_root_mean_squared_error: 1.5619\n",
      "Epoch 14/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 200.4855 - friendly_loss: 40.2572 - trustworthy_loss: 40.0095 - confident_loss: 40.0926 - competent_loss: 40.0428 - wealthy_loss: 40.0834 - friendly_root_mean_squared_error: 1.4877 - trustworthy_root_mean_squared_error: 1.4824 - confident_root_mean_squared_error: 1.4800 - competent_root_mean_squared_error: 1.4867 - wealthy_root_mean_squared_error: 1.4946 - val_loss: 228.0969 - val_friendly_loss: 45.6576 - val_trustworthy_loss: 45.6498 - val_confident_loss: 45.6804 - val_competent_loss: 45.8111 - val_wealthy_loss: 45.2980 - val_friendly_root_mean_squared_error: 1.5079 - val_trustworthy_root_mean_squared_error: 1.5369 - val_confident_root_mean_squared_error: 1.4931 - val_competent_root_mean_squared_error: 1.5202 - val_wealthy_root_mean_squared_error: 1.5402\n",
      "Epoch 15/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 200.1418 - friendly_loss: 40.1725 - trustworthy_loss: 39.8855 - confident_loss: 40.0151 - competent_loss: 40.0260 - wealthy_loss: 40.0427 - friendly_root_mean_squared_error: 1.4831 - trustworthy_root_mean_squared_error: 1.4778 - confident_root_mean_squared_error: 1.4696 - competent_root_mean_squared_error: 1.4754 - wealthy_root_mean_squared_error: 1.4816 - val_loss: 226.2521 - val_friendly_loss: 45.2956 - val_trustworthy_loss: 45.3913 - val_confident_loss: 45.4409 - val_competent_loss: 45.2323 - val_wealthy_loss: 44.8921 - val_friendly_root_mean_squared_error: 1.5097 - val_trustworthy_root_mean_squared_error: 1.4969 - val_confident_root_mean_squared_error: 1.5089 - val_competent_root_mean_squared_error: 1.5219 - val_wealthy_root_mean_squared_error: 1.5282\n",
      "Epoch 16/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 198.1441 - friendly_loss: 39.7757 - trustworthy_loss: 39.5636 - confident_loss: 39.6504 - competent_loss: 39.5766 - wealthy_loss: 39.5778 - friendly_root_mean_squared_error: 1.4786 - trustworthy_root_mean_squared_error: 1.4728 - confident_root_mean_squared_error: 1.4632 - competent_root_mean_squared_error: 1.4770 - wealthy_root_mean_squared_error: 1.4751 - val_loss: 229.6015 - val_friendly_loss: 45.9243 - val_trustworthy_loss: 46.3706 - val_confident_loss: 45.9415 - val_competent_loss: 45.9140 - val_wealthy_loss: 45.4510 - val_friendly_root_mean_squared_error: 1.5188 - val_trustworthy_root_mean_squared_error: 1.5079 - val_confident_root_mean_squared_error: 1.5318 - val_competent_root_mean_squared_error: 1.5168 - val_wealthy_root_mean_squared_error: 1.5268\n",
      "Epoch 17/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 197.8200 - friendly_loss: 39.6781 - trustworthy_loss: 39.4922 - confident_loss: 39.5829 - competent_loss: 39.5662 - wealthy_loss: 39.5005 - friendly_root_mean_squared_error: 1.4724 - trustworthy_root_mean_squared_error: 1.4684 - confident_root_mean_squared_error: 1.4594 - competent_root_mean_squared_error: 1.4736 - wealthy_root_mean_squared_error: 1.4727 - val_loss: 225.4482 - val_friendly_loss: 45.1002 - val_trustworthy_loss: 45.2063 - val_confident_loss: 45.2314 - val_competent_loss: 45.1339 - val_wealthy_loss: 44.7765 - val_friendly_root_mean_squared_error: 1.6046 - val_trustworthy_root_mean_squared_error: 1.6258 - val_confident_root_mean_squared_error: 1.6337 - val_competent_root_mean_squared_error: 1.6374 - val_wealthy_root_mean_squared_error: 1.6372\n",
      "Epoch 18/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 196.4736 - friendly_loss: 39.4378 - trustworthy_loss: 39.1587 - confident_loss: 39.3758 - competent_loss: 39.3163 - wealthy_loss: 39.1850 - friendly_root_mean_squared_error: 1.4733 - trustworthy_root_mean_squared_error: 1.4650 - confident_root_mean_squared_error: 1.4570 - competent_root_mean_squared_error: 1.4665 - wealthy_root_mean_squared_error: 1.4655 - val_loss: 225.8407 - val_friendly_loss: 45.3723 - val_trustworthy_loss: 45.2231 - val_confident_loss: 45.2182 - val_competent_loss: 45.1188 - val_wealthy_loss: 44.9082 - val_friendly_root_mean_squared_error: 1.5536 - val_trustworthy_root_mean_squared_error: 1.5719 - val_confident_root_mean_squared_error: 1.5348 - val_competent_root_mean_squared_error: 1.5805 - val_wealthy_root_mean_squared_error: 1.5370\n",
      "Epoch 19/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 194.7031 - friendly_loss: 39.0517 - trustworthy_loss: 38.8033 - confident_loss: 38.9954 - competent_loss: 38.9683 - wealthy_loss: 38.8845 - friendly_root_mean_squared_error: 1.4643 - trustworthy_root_mean_squared_error: 1.4627 - confident_root_mean_squared_error: 1.4497 - competent_root_mean_squared_error: 1.4641 - wealthy_root_mean_squared_error: 1.4601 - val_loss: 230.0010 - val_friendly_loss: 46.2506 - val_trustworthy_loss: 45.8242 - val_confident_loss: 46.5649 - val_competent_loss: 45.9140 - val_wealthy_loss: 45.4474 - val_friendly_root_mean_squared_error: 1.5147 - val_trustworthy_root_mean_squared_error: 1.5245 - val_confident_root_mean_squared_error: 1.4867 - val_competent_root_mean_squared_error: 1.5154 - val_wealthy_root_mean_squared_error: 1.5099\n",
      "Epoch 20/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 194.6951 - friendly_loss: 39.0498 - trustworthy_loss: 38.8256 - confident_loss: 39.0469 - competent_loss: 38.9426 - wealthy_loss: 38.8301 - friendly_root_mean_squared_error: 1.4663 - trustworthy_root_mean_squared_error: 1.4630 - confident_root_mean_squared_error: 1.4503 - competent_root_mean_squared_error: 1.4610 - wealthy_root_mean_squared_error: 1.4626 - val_loss: 236.0743 - val_friendly_loss: 46.5543 - val_trustworthy_loss: 47.5096 - val_confident_loss: 47.3123 - val_competent_loss: 47.9184 - val_wealthy_loss: 46.7797 - val_friendly_root_mean_squared_error: 1.4856 - val_trustworthy_root_mean_squared_error: 1.4764 - val_confident_root_mean_squared_error: 1.4513 - val_competent_root_mean_squared_error: 1.4509 - val_wealthy_root_mean_squared_error: 1.4686\n",
      "Epoch 21/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 194.1404 - friendly_loss: 38.9147 - trustworthy_loss: 38.7142 - confident_loss: 38.8730 - competent_loss: 38.8324 - wealthy_loss: 38.8061 - friendly_root_mean_squared_error: 1.4693 - trustworthy_root_mean_squared_error: 1.4618 - confident_root_mean_squared_error: 1.4512 - competent_root_mean_squared_error: 1.4671 - wealthy_root_mean_squared_error: 1.4650 - val_loss: 232.7927 - val_friendly_loss: 46.5957 - val_trustworthy_loss: 46.3165 - val_confident_loss: 47.1365 - val_competent_loss: 46.5710 - val_wealthy_loss: 46.1730 - val_friendly_root_mean_squared_error: 1.4567 - val_trustworthy_root_mean_squared_error: 1.4867 - val_confident_root_mean_squared_error: 1.4255 - val_competent_root_mean_squared_error: 1.4684 - val_wealthy_root_mean_squared_error: 1.4490\n",
      "Epoch 22/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 193.1772 - friendly_loss: 38.7924 - trustworthy_loss: 38.5150 - confident_loss: 38.6728 - competent_loss: 38.6352 - wealthy_loss: 38.5617 - friendly_root_mean_squared_error: 1.4630 - trustworthy_root_mean_squared_error: 1.4649 - confident_root_mean_squared_error: 1.4483 - competent_root_mean_squared_error: 1.4591 - wealthy_root_mean_squared_error: 1.4609 - val_loss: 225.4182 - val_friendly_loss: 45.2898 - val_trustworthy_loss: 44.9461 - val_confident_loss: 45.3779 - val_competent_loss: 45.0523 - val_wealthy_loss: 44.7521 - val_friendly_root_mean_squared_error: 1.5772 - val_trustworthy_root_mean_squared_error: 1.5663 - val_confident_root_mean_squared_error: 1.5924 - val_competent_root_mean_squared_error: 1.5731 - val_wealthy_root_mean_squared_error: 1.5847\n",
      "Epoch 23/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 192.1351 - friendly_loss: 38.5251 - trustworthy_loss: 38.3205 - confident_loss: 38.5072 - competent_loss: 38.3903 - wealthy_loss: 38.3920 - friendly_root_mean_squared_error: 1.4600 - trustworthy_root_mean_squared_error: 1.4558 - confident_root_mean_squared_error: 1.4416 - competent_root_mean_squared_error: 1.4613 - wealthy_root_mean_squared_error: 1.4580 - val_loss: 232.9222 - val_friendly_loss: 46.5227 - val_trustworthy_loss: 46.8872 - val_confident_loss: 47.0222 - val_competent_loss: 46.3309 - val_wealthy_loss: 46.1592 - val_friendly_root_mean_squared_error: 1.4742 - val_trustworthy_root_mean_squared_error: 1.4711 - val_confident_root_mean_squared_error: 1.4573 - val_competent_root_mean_squared_error: 1.4839 - val_wealthy_root_mean_squared_error: 1.4787\n",
      "Epoch 24/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 190.8627 - friendly_loss: 38.2922 - trustworthy_loss: 38.1024 - confident_loss: 38.2901 - competent_loss: 38.1125 - wealthy_loss: 38.0654 - friendly_root_mean_squared_error: 1.4442 - trustworthy_root_mean_squared_error: 1.4475 - confident_root_mean_squared_error: 1.4335 - competent_root_mean_squared_error: 1.4404 - wealthy_root_mean_squared_error: 1.4461 - val_loss: 232.0352 - val_friendly_loss: 46.5137 - val_trustworthy_loss: 46.1485 - val_confident_loss: 46.7758 - val_competent_loss: 46.7645 - val_wealthy_loss: 45.8328 - val_friendly_root_mean_squared_error: 1.4869 - val_trustworthy_root_mean_squared_error: 1.5066 - val_confident_root_mean_squared_error: 1.4834 - val_competent_root_mean_squared_error: 1.4898 - val_wealthy_root_mean_squared_error: 1.5134\n",
      "Epoch 25/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 190.8745 - friendly_loss: 38.2925 - trustworthy_loss: 38.0469 - confident_loss: 38.3085 - competent_loss: 38.1443 - wealthy_loss: 38.0823 - friendly_root_mean_squared_error: 1.4533 - trustworthy_root_mean_squared_error: 1.4509 - confident_root_mean_squared_error: 1.4414 - competent_root_mean_squared_error: 1.4551 - wealthy_root_mean_squared_error: 1.4552 - val_loss: 225.4993 - val_friendly_loss: 45.0693 - val_trustworthy_loss: 45.0456 - val_confident_loss: 45.2402 - val_competent_loss: 45.1633 - val_wealthy_loss: 44.9809 - val_friendly_root_mean_squared_error: 1.6036 - val_trustworthy_root_mean_squared_error: 1.6395 - val_confident_root_mean_squared_error: 1.6012 - val_competent_root_mean_squared_error: 1.5765 - val_wealthy_root_mean_squared_error: 1.6440\n",
      "Epoch 26/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 189.6848 - friendly_loss: 38.0177 - trustworthy_loss: 37.8895 - confident_loss: 38.0597 - competent_loss: 37.8384 - wealthy_loss: 37.8795 - friendly_root_mean_squared_error: 1.4528 - trustworthy_root_mean_squared_error: 1.4488 - confident_root_mean_squared_error: 1.4400 - competent_root_mean_squared_error: 1.4527 - wealthy_root_mean_squared_error: 1.4599 - val_loss: 229.2461 - val_friendly_loss: 45.9208 - val_trustworthy_loss: 45.5499 - val_confident_loss: 46.1077 - val_competent_loss: 45.9877 - val_wealthy_loss: 45.6799 - val_friendly_root_mean_squared_error: 1.5646 - val_trustworthy_root_mean_squared_error: 1.5796 - val_confident_root_mean_squared_error: 1.5357 - val_competent_root_mean_squared_error: 1.5653 - val_wealthy_root_mean_squared_error: 1.5563\n",
      "Epoch 27/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 188.6822 - friendly_loss: 37.7785 - trustworthy_loss: 37.6651 - confident_loss: 37.8633 - competent_loss: 37.7034 - wealthy_loss: 37.6718 - friendly_root_mean_squared_error: 1.4406 - trustworthy_root_mean_squared_error: 1.4434 - confident_root_mean_squared_error: 1.4334 - competent_root_mean_squared_error: 1.4490 - wealthy_root_mean_squared_error: 1.4501 - val_loss: 229.2210 - val_friendly_loss: 46.1524 - val_trustworthy_loss: 45.7655 - val_confident_loss: 45.8557 - val_competent_loss: 45.6915 - val_wealthy_loss: 45.7559 - val_friendly_root_mean_squared_error: 1.4902 - val_trustworthy_root_mean_squared_error: 1.5375 - val_confident_root_mean_squared_error: 1.5397 - val_competent_root_mean_squared_error: 1.5461 - val_wealthy_root_mean_squared_error: 1.5118\n",
      "Epoch 28/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 187.8397 - friendly_loss: 37.5905 - trustworthy_loss: 37.4953 - confident_loss: 37.7115 - competent_loss: 37.5080 - wealthy_loss: 37.5345 - friendly_root_mean_squared_error: 1.4317 - trustworthy_root_mean_squared_error: 1.4331 - confident_root_mean_squared_error: 1.4253 - competent_root_mean_squared_error: 1.4360 - wealthy_root_mean_squared_error: 1.4354 - val_loss: 226.9671 - val_friendly_loss: 45.4866 - val_trustworthy_loss: 45.5198 - val_confident_loss: 45.7234 - val_competent_loss: 45.4885 - val_wealthy_loss: 44.7489 - val_friendly_root_mean_squared_error: 1.5787 - val_trustworthy_root_mean_squared_error: 1.5302 - val_confident_root_mean_squared_error: 1.5373 - val_competent_root_mean_squared_error: 1.5540 - val_wealthy_root_mean_squared_error: 1.5723\n",
      "Epoch 29/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 186.8805 - friendly_loss: 37.4842 - trustworthy_loss: 37.2497 - confident_loss: 37.5144 - competent_loss: 37.3236 - wealthy_loss: 37.3085 - friendly_root_mean_squared_error: 1.4382 - trustworthy_root_mean_squared_error: 1.4355 - confident_root_mean_squared_error: 1.4235 - competent_root_mean_squared_error: 1.4366 - wealthy_root_mean_squared_error: 1.4364 - val_loss: 230.0993 - val_friendly_loss: 46.1582 - val_trustworthy_loss: 45.8911 - val_confident_loss: 46.1592 - val_competent_loss: 46.0198 - val_wealthy_loss: 45.8710 - val_friendly_root_mean_squared_error: 1.6861 - val_trustworthy_root_mean_squared_error: 1.6985 - val_confident_root_mean_squared_error: 1.6980 - val_competent_root_mean_squared_error: 1.7039 - val_wealthy_root_mean_squared_error: 1.6979\n",
      "Epoch 30/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 186.4617 - friendly_loss: 37.3789 - trustworthy_loss: 37.1760 - confident_loss: 37.4122 - competent_loss: 37.2803 - wealthy_loss: 37.2143 - friendly_root_mean_squared_error: 1.4302 - trustworthy_root_mean_squared_error: 1.4347 - confident_root_mean_squared_error: 1.4240 - competent_root_mean_squared_error: 1.4411 - wealthy_root_mean_squared_error: 1.4387 - val_loss: 228.4304 - val_friendly_loss: 45.5556 - val_trustworthy_loss: 45.5823 - val_confident_loss: 45.7441 - val_competent_loss: 45.8643 - val_wealthy_loss: 45.6841 - val_friendly_root_mean_squared_error: 1.5879 - val_trustworthy_root_mean_squared_error: 1.6130 - val_confident_root_mean_squared_error: 1.5665 - val_competent_root_mean_squared_error: 1.6141 - val_wealthy_root_mean_squared_error: 1.6104\n",
      "Epoch 31/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 185.8291 - friendly_loss: 37.1872 - trustworthy_loss: 37.0875 - confident_loss: 37.3134 - competent_loss: 37.1320 - wealthy_loss: 37.1090 - friendly_root_mean_squared_error: 1.4316 - trustworthy_root_mean_squared_error: 1.4268 - confident_root_mean_squared_error: 1.4209 - competent_root_mean_squared_error: 1.4380 - wealthy_root_mean_squared_error: 1.4383 - val_loss: 229.9735 - val_friendly_loss: 46.0324 - val_trustworthy_loss: 45.9740 - val_confident_loss: 46.3632 - val_competent_loss: 45.9690 - val_wealthy_loss: 45.6348 - val_friendly_root_mean_squared_error: 1.6620 - val_trustworthy_root_mean_squared_error: 1.6969 - val_confident_root_mean_squared_error: 1.6681 - val_competent_root_mean_squared_error: 1.6527 - val_wealthy_root_mean_squared_error: 1.6609\n",
      "Epoch 32/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 185.0744 - friendly_loss: 37.0893 - trustworthy_loss: 36.9180 - confident_loss: 37.1566 - competent_loss: 36.9884 - wealthy_loss: 36.9221 - friendly_root_mean_squared_error: 1.4289 - trustworthy_root_mean_squared_error: 1.4334 - confident_root_mean_squared_error: 1.4193 - competent_root_mean_squared_error: 1.4324 - wealthy_root_mean_squared_error: 1.4308 - val_loss: 237.5663 - val_friendly_loss: 47.3595 - val_trustworthy_loss: 47.7005 - val_confident_loss: 47.5156 - val_competent_loss: 47.7044 - val_wealthy_loss: 47.2864 - val_friendly_root_mean_squared_error: 1.4843 - val_trustworthy_root_mean_squared_error: 1.4739 - val_confident_root_mean_squared_error: 1.4943 - val_competent_root_mean_squared_error: 1.4804 - val_wealthy_root_mean_squared_error: 1.4813\n",
      "Epoch 33/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 184.6061 - friendly_loss: 36.9810 - trustworthy_loss: 36.8685 - confident_loss: 37.0933 - competent_loss: 36.9053 - wealthy_loss: 36.7580 - friendly_root_mean_squared_error: 1.4205 - trustworthy_root_mean_squared_error: 1.4268 - confident_root_mean_squared_error: 1.4100 - competent_root_mean_squared_error: 1.4240 - wealthy_root_mean_squared_error: 1.4214 - val_loss: 230.0713 - val_friendly_loss: 46.4392 - val_trustworthy_loss: 45.8685 - val_confident_loss: 46.0411 - val_competent_loss: 45.9341 - val_wealthy_loss: 45.7884 - val_friendly_root_mean_squared_error: 1.5349 - val_trustworthy_root_mean_squared_error: 1.5683 - val_confident_root_mean_squared_error: 1.5625 - val_competent_root_mean_squared_error: 1.5520 - val_wealthy_root_mean_squared_error: 1.6169\n",
      "Epoch 34/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 183.4585 - friendly_loss: 36.7670 - trustworthy_loss: 36.5511 - confident_loss: 36.9223 - competent_loss: 36.6485 - wealthy_loss: 36.5695 - friendly_root_mean_squared_error: 1.4215 - trustworthy_root_mean_squared_error: 1.4233 - confident_root_mean_squared_error: 1.4191 - competent_root_mean_squared_error: 1.4320 - wealthy_root_mean_squared_error: 1.4297 - val_loss: 227.8223 - val_friendly_loss: 45.6875 - val_trustworthy_loss: 45.5023 - val_confident_loss: 45.6472 - val_competent_loss: 45.6256 - val_wealthy_loss: 45.3597 - val_friendly_root_mean_squared_error: 1.6201 - val_trustworthy_root_mean_squared_error: 1.5964 - val_confident_root_mean_squared_error: 1.5913 - val_competent_root_mean_squared_error: 1.5922 - val_wealthy_root_mean_squared_error: 1.6097\n",
      "Epoch 35/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 183.7978 - friendly_loss: 36.7491 - trustworthy_loss: 36.6727 - confident_loss: 36.9640 - competent_loss: 36.7571 - wealthy_loss: 36.6551 - friendly_root_mean_squared_error: 1.4242 - trustworthy_root_mean_squared_error: 1.4244 - confident_root_mean_squared_error: 1.4137 - competent_root_mean_squared_error: 1.4257 - wealthy_root_mean_squared_error: 1.4303 - val_loss: 228.8523 - val_friendly_loss: 46.0500 - val_trustworthy_loss: 45.7970 - val_confident_loss: 45.7306 - val_competent_loss: 45.9154 - val_wealthy_loss: 45.3593 - val_friendly_root_mean_squared_error: 1.5535 - val_trustworthy_root_mean_squared_error: 1.5380 - val_confident_root_mean_squared_error: 1.5298 - val_competent_root_mean_squared_error: 1.5659 - val_wealthy_root_mean_squared_error: 1.5758\n",
      "Epoch 36/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 182.6731 - friendly_loss: 36.5820 - trustworthy_loss: 36.4883 - confident_loss: 36.7359 - competent_loss: 36.4348 - wealthy_loss: 36.4322 - friendly_root_mean_squared_error: 1.4212 - trustworthy_root_mean_squared_error: 1.4196 - confident_root_mean_squared_error: 1.4087 - competent_root_mean_squared_error: 1.4187 - wealthy_root_mean_squared_error: 1.4225 - val_loss: 228.4190 - val_friendly_loss: 45.7351 - val_trustworthy_loss: 45.4814 - val_confident_loss: 45.9617 - val_competent_loss: 45.5648 - val_wealthy_loss: 45.6758 - val_friendly_root_mean_squared_error: 1.5880 - val_trustworthy_root_mean_squared_error: 1.6038 - val_confident_root_mean_squared_error: 1.5520 - val_competent_root_mean_squared_error: 1.5687 - val_wealthy_root_mean_squared_error: 1.5502\n",
      "Epoch 37/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 182.2612 - friendly_loss: 36.4558 - trustworthy_loss: 36.3670 - confident_loss: 36.6918 - competent_loss: 36.3672 - wealthy_loss: 36.3793 - friendly_root_mean_squared_error: 1.4165 - trustworthy_root_mean_squared_error: 1.4142 - confident_root_mean_squared_error: 1.4048 - competent_root_mean_squared_error: 1.4208 - wealthy_root_mean_squared_error: 1.4223 - val_loss: 229.2890 - val_friendly_loss: 45.8061 - val_trustworthy_loss: 45.7432 - val_confident_loss: 46.0809 - val_competent_loss: 45.9946 - val_wealthy_loss: 45.6643 - val_friendly_root_mean_squared_error: 1.6245 - val_trustworthy_root_mean_squared_error: 1.6181 - val_confident_root_mean_squared_error: 1.5740 - val_competent_root_mean_squared_error: 1.6286 - val_wealthy_root_mean_squared_error: 1.6203\n",
      "Epoch 38/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 182.1292 - friendly_loss: 36.4088 - trustworthy_loss: 36.3831 - confident_loss: 36.5718 - competent_loss: 36.4484 - wealthy_loss: 36.3171 - friendly_root_mean_squared_error: 1.4185 - trustworthy_root_mean_squared_error: 1.4198 - confident_root_mean_squared_error: 1.4069 - competent_root_mean_squared_error: 1.4248 - wealthy_root_mean_squared_error: 1.4240 - val_loss: 231.0852 - val_friendly_loss: 46.3043 - val_trustworthy_loss: 46.1322 - val_confident_loss: 46.7086 - val_competent_loss: 45.8752 - val_wealthy_loss: 46.0649 - val_friendly_root_mean_squared_error: 1.5874 - val_trustworthy_root_mean_squared_error: 1.5567 - val_confident_root_mean_squared_error: 1.5705 - val_competent_root_mean_squared_error: 1.5743 - val_wealthy_root_mean_squared_error: 1.5804\n",
      "Epoch 39/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 181.1734 - friendly_loss: 36.2562 - trustworthy_loss: 36.0750 - confident_loss: 36.4528 - competent_loss: 36.2054 - wealthy_loss: 36.1841 - friendly_root_mean_squared_error: 1.4139 - trustworthy_root_mean_squared_error: 1.4128 - confident_root_mean_squared_error: 1.4028 - competent_root_mean_squared_error: 1.4175 - wealthy_root_mean_squared_error: 1.4182 - val_loss: 229.0509 - val_friendly_loss: 46.0836 - val_trustworthy_loss: 45.3346 - val_confident_loss: 45.9949 - val_competent_loss: 45.8469 - val_wealthy_loss: 45.7909 - val_friendly_root_mean_squared_error: 1.5846 - val_trustworthy_root_mean_squared_error: 1.6184 - val_confident_root_mean_squared_error: 1.5589 - val_competent_root_mean_squared_error: 1.5630 - val_wealthy_root_mean_squared_error: 1.6024\n",
      "Epoch 40/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 180.4445 - friendly_loss: 36.1060 - trustworthy_loss: 35.9972 - confident_loss: 36.3411 - competent_loss: 36.0145 - wealthy_loss: 35.9858 - friendly_root_mean_squared_error: 1.4093 - trustworthy_root_mean_squared_error: 1.4180 - confident_root_mean_squared_error: 1.4039 - competent_root_mean_squared_error: 1.4147 - wealthy_root_mean_squared_error: 1.4165 - val_loss: 231.8825 - val_friendly_loss: 46.3820 - val_trustworthy_loss: 46.1884 - val_confident_loss: 46.6169 - val_competent_loss: 46.1759 - val_wealthy_loss: 46.5193 - val_friendly_root_mean_squared_error: 1.5726 - val_trustworthy_root_mean_squared_error: 1.5411 - val_confident_root_mean_squared_error: 1.5283 - val_competent_root_mean_squared_error: 1.5778 - val_wealthy_root_mean_squared_error: 1.5552\n",
      "Epoch 41/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 180.0258 - friendly_loss: 36.0090 - trustworthy_loss: 35.8536 - confident_loss: 36.3081 - competent_loss: 35.8923 - wealthy_loss: 35.9627 - friendly_root_mean_squared_error: 1.4137 - trustworthy_root_mean_squared_error: 1.4071 - confident_root_mean_squared_error: 1.4002 - competent_root_mean_squared_error: 1.4109 - wealthy_root_mean_squared_error: 1.4097 - val_loss: 232.5851 - val_friendly_loss: 46.7229 - val_trustworthy_loss: 46.6438 - val_confident_loss: 46.8576 - val_competent_loss: 46.2707 - val_wealthy_loss: 46.0901 - val_friendly_root_mean_squared_error: 1.5762 - val_trustworthy_root_mean_squared_error: 1.5695 - val_confident_root_mean_squared_error: 1.5237 - val_competent_root_mean_squared_error: 1.5858 - val_wealthy_root_mean_squared_error: 1.5939\n",
      "Epoch 42/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 180.1110 - friendly_loss: 36.0497 - trustworthy_loss: 35.9261 - confident_loss: 36.2297 - competent_loss: 36.0069 - wealthy_loss: 35.8986 - friendly_root_mean_squared_error: 1.4119 - trustworthy_root_mean_squared_error: 1.4095 - confident_root_mean_squared_error: 1.3967 - competent_root_mean_squared_error: 1.4133 - wealthy_root_mean_squared_error: 1.4159 - val_loss: 229.6471 - val_friendly_loss: 46.0262 - val_trustworthy_loss: 45.4373 - val_confident_loss: 46.3476 - val_competent_loss: 45.8901 - val_wealthy_loss: 45.9459 - val_friendly_root_mean_squared_error: 1.5948 - val_trustworthy_root_mean_squared_error: 1.6092 - val_confident_root_mean_squared_error: 1.6088 - val_competent_root_mean_squared_error: 1.6183 - val_wealthy_root_mean_squared_error: 1.6057\n",
      "Epoch 43/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 179.5999 - friendly_loss: 35.9129 - trustworthy_loss: 35.7769 - confident_loss: 36.1533 - competent_loss: 35.8662 - wealthy_loss: 35.8906 - friendly_root_mean_squared_error: 1.4133 - trustworthy_root_mean_squared_error: 1.4126 - confident_root_mean_squared_error: 1.4040 - competent_root_mean_squared_error: 1.4097 - wealthy_root_mean_squared_error: 1.4166 - val_loss: 233.9284 - val_friendly_loss: 46.6829 - val_trustworthy_loss: 46.5986 - val_confident_loss: 47.5882 - val_competent_loss: 46.6091 - val_wealthy_loss: 46.4497 - val_friendly_root_mean_squared_error: 1.5234 - val_trustworthy_root_mean_squared_error: 1.5203 - val_confident_root_mean_squared_error: 1.4928 - val_competent_root_mean_squared_error: 1.5208 - val_wealthy_root_mean_squared_error: 1.5326\n",
      "Epoch 44/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 178.8072 - friendly_loss: 35.7559 - trustworthy_loss: 35.6725 - confident_loss: 35.9693 - competent_loss: 35.6718 - wealthy_loss: 35.7378 - friendly_root_mean_squared_error: 1.4070 - trustworthy_root_mean_squared_error: 1.4033 - confident_root_mean_squared_error: 1.3965 - competent_root_mean_squared_error: 1.4053 - wealthy_root_mean_squared_error: 1.4086 - val_loss: 229.9809 - val_friendly_loss: 45.8846 - val_trustworthy_loss: 45.8821 - val_confident_loss: 46.4442 - val_competent_loss: 46.1446 - val_wealthy_loss: 45.6253 - val_friendly_root_mean_squared_error: 1.6449 - val_trustworthy_root_mean_squared_error: 1.6295 - val_confident_root_mean_squared_error: 1.6076 - val_competent_root_mean_squared_error: 1.6415 - val_wealthy_root_mean_squared_error: 1.5931\n",
      "Epoch 45/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 178.6268 - friendly_loss: 35.7488 - trustworthy_loss: 35.5436 - confident_loss: 36.0567 - competent_loss: 35.6422 - wealthy_loss: 35.6355 - friendly_root_mean_squared_error: 1.4123 - trustworthy_root_mean_squared_error: 1.4087 - confident_root_mean_squared_error: 1.3984 - competent_root_mean_squared_error: 1.4053 - wealthy_root_mean_squared_error: 1.4156 - val_loss: 232.7902 - val_friendly_loss: 46.2650 - val_trustworthy_loss: 46.2234 - val_confident_loss: 46.8842 - val_competent_loss: 46.5445 - val_wealthy_loss: 46.8731 - val_friendly_root_mean_squared_error: 1.5624 - val_trustworthy_root_mean_squared_error: 1.5581 - val_confident_root_mean_squared_error: 1.5345 - val_competent_root_mean_squared_error: 1.5536 - val_wealthy_root_mean_squared_error: 1.5476\n",
      "Epoch 46/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 178.4470 - friendly_loss: 35.7289 - trustworthy_loss: 35.5339 - confident_loss: 35.9853 - competent_loss: 35.5880 - wealthy_loss: 35.6108 - friendly_root_mean_squared_error: 1.4077 - trustworthy_root_mean_squared_error: 1.4071 - confident_root_mean_squared_error: 1.4012 - competent_root_mean_squared_error: 1.4037 - wealthy_root_mean_squared_error: 1.4161 - val_loss: 230.3328 - val_friendly_loss: 46.1073 - val_trustworthy_loss: 45.9715 - val_confident_loss: 46.3303 - val_competent_loss: 45.9841 - val_wealthy_loss: 45.9397 - val_friendly_root_mean_squared_error: 1.5994 - val_trustworthy_root_mean_squared_error: 1.5660 - val_confident_root_mean_squared_error: 1.6245 - val_competent_root_mean_squared_error: 1.6161 - val_wealthy_root_mean_squared_error: 1.6190\n",
      "Epoch 47/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 178.2264 - friendly_loss: 35.6586 - trustworthy_loss: 35.4315 - confident_loss: 35.9675 - competent_loss: 35.6313 - wealthy_loss: 35.5375 - friendly_root_mean_squared_error: 1.4038 - trustworthy_root_mean_squared_error: 1.4049 - confident_root_mean_squared_error: 1.3979 - competent_root_mean_squared_error: 1.4043 - wealthy_root_mean_squared_error: 1.4067 - val_loss: 230.5973 - val_friendly_loss: 46.2329 - val_trustworthy_loss: 45.9608 - val_confident_loss: 46.3840 - val_competent_loss: 45.9495 - val_wealthy_loss: 46.0702 - val_friendly_root_mean_squared_error: 1.5252 - val_trustworthy_root_mean_squared_error: 1.5445 - val_confident_root_mean_squared_error: 1.5405 - val_competent_root_mean_squared_error: 1.5786 - val_wealthy_root_mean_squared_error: 1.5396\n",
      "Epoch 48/48\n",
      "351/351 [==============================] - 1s 2ms/step - loss: 177.7713 - friendly_loss: 35.5202 - trustworthy_loss: 35.4057 - confident_loss: 35.8514 - competent_loss: 35.4720 - wealthy_loss: 35.5220 - friendly_root_mean_squared_error: 1.4020 - trustworthy_root_mean_squared_error: 1.4012 - confident_root_mean_squared_error: 1.3936 - competent_root_mean_squared_error: 1.4053 - wealthy_root_mean_squared_error: 1.4064 - val_loss: 231.3164 - val_friendly_loss: 46.1289 - val_trustworthy_loss: 46.2530 - val_confident_loss: 46.5938 - val_competent_loss: 46.2530 - val_wealthy_loss: 46.0876 - val_friendly_root_mean_squared_error: 1.5400 - val_trustworthy_root_mean_squared_error: 1.5269 - val_confident_root_mean_squared_error: 1.5213 - val_competent_root_mean_squared_error: 1.5388 - val_wealthy_root_mean_squared_error: 1.5233\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 231.3164 - friendly_loss: 46.1289 - trustworthy_loss: 46.2530 - confident_loss: 46.5938 - competent_loss: 46.2530 - wealthy_loss: 46.0876 - friendly_root_mean_squared_error: 1.5400 - trustworthy_root_mean_squared_error: 1.5269 - confident_root_mean_squared_error: 1.5213 - competent_root_mean_squared_error: 1.5388 - wealthy_root_mean_squared_error: 1.5233\n",
      "[231.31642150878906, 46.128910064697266, 46.25299072265625, 46.59382629394531, 46.25303649902344, 46.08761215209961, 1.5400482416152954, 1.5268834829330444, 1.5213350057601929, 1.5387673377990723, 1.5233330726623535]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanAbsolutePercentageError(reduction=\"auto\"),\n",
    "              metrics=tf.keras.metrics.RootMeanSquaredError())\n",
    "\n",
    "history = model.fit(train_ds, epochs=48, validation_data=test_ds)\n",
    "evaluation = model.evaluate(test_ds)\n",
    "print(evaluation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try with embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the model"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
