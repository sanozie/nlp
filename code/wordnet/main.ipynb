{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4395 NLP\n",
    "## WordNet Assignment\n",
    "### Samuel Anozie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a practical data set of words, including specific word types like nouns, verbs adjectives, and adverbs. Synonyms are grouped together in sets called synsets, and are organized in a hierarchical tree of semantics. Originally created to simulate the way humans theoretically understand the relationships between different words, WordNet is a very useful tool in the exploration of the English language with machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('power.n.01'), Synset('power.n.02'), Synset('ability.n.02'), Synset('office.n.04'), Synset('power.n.05'), Synset('exponent.n.03'), Synset('might.n.01'), Synset('world_power.n.01'), Synset('baron.n.03'), Synset('power.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "word = \"power\"\n",
    "synsets = wn.synsets(word)\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet nouns are organized in a hierarchy, with the top noun defaulting to 'entity'. There can be many levels of abstraction from the word 'entity' to get to a final word. In this particular example, there are 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possession of the qualities (especially mental qualities) required to do something or get something done\n",
      "[Lemma('ability.n.02.ability'), Lemma('ability.n.02.power')]\n",
      "['danger heightened his powers of discrimination']\n"
     ]
    }
   ],
   "source": [
    "picked_synset = synsets[2]\n",
    "print(picked_synset.definition())\n",
    "print(picked_synset.lemmas())\n",
    "print(picked_synset.examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cognition.n.01')\n",
      "Synset('psychological_feature.n.01')\n",
      "Synset('abstraction.n.06')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "hyp = picked_synset.hypernyms()[0]\n",
    "top = wn.synset('entity.n.01')\n",
    "\n",
    "while hyp:\n",
    "    print(hyp)\n",
    "    if hyp == top:\n",
    "        break\n",
    "    if hyp.hypernyms():\n",
    "        hyp = hyp.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cognition.n.01')]\n",
      "[Synset('aptitude.n.01'), Synset('bilingualism.n.01'), Synset('capacity.n.08'), Synset('creativity.n.01'), Synset('faculty.n.01'), Synset('hand.n.04'), Synset('intelligence.n.01'), Synset('know-how.n.01'), Synset('leadership.n.04'), Synset('originality.n.01'), Synset('skill.n.01'), Synset('skill.n.02'), Synset('superior_skill.n.01')]\n",
      "[]\n",
      "[]\n",
      "Lemma('inability.n.01.inability')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(picked_synset.hypernyms())\n",
    "print(picked_synset.hyponyms())\n",
    "print(picked_synset.part_meronyms())\n",
    "print(picked_synset.part_holonyms())\n",
    "for lemma in picked_synset.lemmas():\n",
    "    print(*lemma.antonyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbs are handled in WordNet sligntly differently than nouns. Instead of one common hierarchical ancestor, each verb is not guarenteed to have the same ancestors. There are various different root words that derive more verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('choice.n.01'), Synset('picking.n.01'), Synset('cream.n.01'), Synset('woof.n.01'), Synset('pick.n.05'), Synset('pick.n.06'), Synset('pick.n.07'), Synset('pick.n.08'), Synset('choice.n.02'), Synset('pick.v.01'), Synset('pick.v.02'), Synset('blame.v.02'), Synset('pick.v.04'), Synset('pick.v.05'), Synset('clean.v.02'), Synset('pick.v.07'), Synset('foot.v.01'), Synset('pluck.v.04'), Synset('pick.v.10'), Synset('peck.v.01'), Synset('nibble.v.03')]\n"
     ]
    }
   ],
   "source": [
    "word = \"pick\"\n",
    "synsets = wn.synsets(word)\n",
    "print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pull lightly but sharply with a plucking motion\n",
      "[Lemma('pluck.v.04.pluck'), Lemma('pluck.v.04.plunk'), Lemma('pluck.v.04.pick')]\n",
      "['he plucked the strings of his mandolin']\n"
     ]
    }
   ],
   "source": [
    "picked_synset = synsets[17]\n",
    "print(picked_synset.definition())\n",
    "print(picked_synset.lemmas())\n",
    "print(picked_synset.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('pull.v.01')\n",
      "Synset('move.v.02')\n"
     ]
    }
   ],
   "source": [
    "hyp = picked_synset.hypernyms()[0]\n",
    "top = picked_synset.root_hypernyms()[0]\n",
    "while hyp:\n",
    "    print(hyp)\n",
    "    if hyp == top:\n",
    "        break\n",
    "    if hyp.hypernyms():\n",
    "        hyp = hyp.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pick'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy(word, wn.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wu-Palmer similarity metric defines the level of similarity between two different words on a scale of 0 to 1, with the higher number meaning similar words. The Lesk algorithm, on the other hand, seeks to remove ambiguity for the meanings of certain words by analyting their context. Even though the algorithm is dwarfed by more modern word sense disambiguation processes, it is a lexical foundation that can inform future implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Synset('spirit.n.02')\n",
      "Synset('watch.v.05')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "look = wn.synsets(\"look\")[4]\n",
    "watch = wn.synsets(\"watch\")[6]\n",
    "print(wn.wup_similarity(look, watch))\n",
    "\n",
    "look_sent = \"I want to look at the sky\"\n",
    "watch_sent = \"I want to watch the clouds\"\n",
    "\n",
    "print(lesk(look_sent.split(), 'look', 'n'))\n",
    "print(lesk(watch_sent.split(), 'watch', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SentiWordNet is one of the more interesting parts of the NLTK corpus. Similar to the WordNet, it is a database of words that include sentiment scores as part of each synset: positivity, negativity, and objectivity. For tasks that need to respond to the sentiment of a sentence instead of just the content, this package is invaluable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<love.n.01: PosScore=0.625 NegScore=0.0>\n",
      "Positive score =  0.625\n",
      "Negative score =  0.0\n",
      "Objective score =  0.375\n",
      "neg\tpos counts\n",
      "0.0 \t 1.25\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "breakdown = swn.senti_synset('love.n.01')\n",
    "print(breakdown)\n",
    "print(\"Positive score = \", breakdown.pos_score())\n",
    "print(\"Negative score = \", breakdown.neg_score())\n",
    "print(\"Objective score = \", breakdown.obj_score())\n",
    "\n",
    "sent = \"I really love cake\"\n",
    "neg = 0\n",
    "pos = 0\n",
    "for token in sent.split():\n",
    "    syn_list = list(swn.senti_synsets(token))\n",
    "    if syn_list:\n",
    "        syn = syn_list[0]\n",
    "        neg += syn.neg_score()\n",
    "        pos += syn.pos_score()\n",
    "\n",
    "print(\"neg\\tpos counts\")\n",
    "print(neg, '\\t', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that words that are next to each other should not be treated independently, but instead, as a pair. In the below example, we see that words like United States, one another, and Indian tribes only have accurate meanings when they are put together, and would mean different things if they were seperated. For tese cases, collocations are derived using the probability of two words occurring next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "6.197700309293973\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from nltk.book import text4\n",
    "\n",
    "text4.collocations()\n",
    "pre = \"foreign\"\n",
    "post = \"nations\"\n",
    "pre_count = 0\n",
    "post_count = 0\n",
    "both_count = 0\n",
    "pre_hit = False\n",
    "\n",
    "for token in text4.tokens:\n",
    "    if token == pre:\n",
    "        pre_count += 1\n",
    "        pre_hit = True\n",
    "    elif token == post and pre_hit:\n",
    "        post_count += 1\n",
    "        both_count += 1\n",
    "        pre_hit = False\n",
    "    elif token == post:\n",
    "        post_count += 1\n",
    "        pre_hit = False\n",
    "\n",
    "mi = math.log((both_count / len(text4.tokens) / ((pre_count / len(text4.tokens)) * (post_count / len(text4.tokens)))))\n",
    "print(mi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
